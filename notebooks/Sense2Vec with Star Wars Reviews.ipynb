{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sense2Vec with Star Wars Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll be running Sense2Vec on the Star Wars Reviews that I have collected. This notebook is based loosely on the article 'Sense2vec with spaCy and Gensim' found at https://explosion.ai/blog/sense2vec-with-spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = pd.read_csv(\"../data/imdb_data.csv\")\n",
    "rt_data_1 = pd.read_csv(\"../rotten_tomatoes/reviews.csv\")\n",
    "rt_data_2 = pd.read_csv(\"../rotten_tomatoes/reviews.csv\")\n",
    "\n",
    "rt_data = pd.concat([rt_data_1, rt_data_2]).drop_duplicates()\n",
    "\n",
    "imdb_data['star_rating'] = imdb_data.star_rating / 2\n",
    "imdb_data['source'] = 'imdb'\n",
    "rt_data['source'] = 'rotten tomatoes'\n",
    "\n",
    "data_cols = ['date', 'name', 'user_link', 'source', 'review', 'downvotes', 'upvotes', 'star_rating']\n",
    "review_data = pd.concat([imdb_data, rt_data])[data_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_texts(texts, nlp):\n",
    "    # Load the annotation models\n",
    "    # Stream texts through the models. We accumulate a buffer and release\n",
    "    # the GIL around the parser, for efficient multi-threading.\n",
    "    skip_count = 0\n",
    "    for i, doc in enumerate(nlp.pipe(texts, n_threads = 4)):\n",
    "        print(\"\\rProcessing %d out of %-30d\" % (i, len(texts)), end = \"\")\n",
    "        \n",
    "        # Iterate over base NPs, e.g. \"all their good ideas\"\n",
    "        for np in list(doc.noun_chunks):\n",
    "            # Only keep adjectives and nouns, e.g. \"good ideas\"\n",
    "            while np.end - np.start > 1 and np[0].dep_ not in ('amod', 'compound'):\n",
    "                np = np[1:]\n",
    "            if np.end - np.start > 1:\n",
    "                # Merge the tokens, e.g. good_ideas\n",
    "                np.merge(tag = np.root.tag_, lemma = np.text, ent_type = np.root.ent_type_)\n",
    "            # Iterate over named entities\n",
    "            for ent in doc.ents:\n",
    "                if len(ent) > 1:\n",
    "                    # Merge them into single tokens\n",
    "                    ent.merge(tag = ent.root.tag_, lemma = ent.text, ent_type = ent.label_)\n",
    "        token_strings = []\n",
    "        for token in doc:\n",
    "            text = token.text.replace(' ', '_')\n",
    "            tag = token.ent_type_ or token.pos_\n",
    "            token_strings.append('%s|%s' % (text, tag))\n",
    "        yield ' '.join(token_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 out of 4368                          "
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Error calculating span: Can't find end",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-a7b97f8e04f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp_reviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-f8dfa1b1e7d2>\u001b[0m in \u001b[0;36mtransform_texts\u001b[1;34m(texts, nlp)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;31m# Only keep adjectives and nouns, e.g. \"good ideas\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdep_\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'amod'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'compound'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                 \u001b[0mnp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span._recalculate_indices\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Error calculating span: Can't find end"
     ]
    }
   ],
   "source": [
    "nlp_reviews = list(transform_texts(review_data.review, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function merge:\n",
      "\n",
      "merge(...) method of spacy.tokens.span.Span instance\n",
      "    Retokenize the document, such that the span is merged into a single\n",
      "    token.\n",
      "    \n",
      "    **attributes: Attributes to assign to the merged token. By default,\n",
      "        attributes are inherited from the syntactic root token of the span.\n",
      "    RETURNS (Token): The newly merged token.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With thunderous laughter, and disappointment.I have no idea how this made it through scripting, story boards, and early screenings. With no one once standing up and saying \"What the hell is this crap? This isn't Star Wars! This comes off as a Comedy Central special, and no Star Wars fan will ever accept this as an acceptable story\"I'm sure some youngster, that has never seen another Star Wars movie, is probably going to watch this, and think it's the best movie they have ever seen. But they will grow up, and go back and watch the real story from Episode 1 through to Episode 7, then get back to this one and ask themselves \"What Happened\".Not the worst movie of all time, but in my opinion, not worthy of existing in the Star Wars universe. This should just be the end of a 2 part arc. Wow... just wow. JJ has a lot of work to do if he's going to try to somehow put all the tooth paste back in the tube after this mess.\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comedy Central\n",
      "story\"I'm\n",
      "Star Wars\n",
      "Episode 1\n",
      "Episode 7\n",
      "Star Wars\n",
      "2\n",
      "JJ\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
